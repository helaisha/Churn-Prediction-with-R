# Load libraries
library(caret)
library(dplyr)
library(readr)
library(skimr)
library(ggplot2)
library(reshape2)
library(glmnet)
library(e1071)  
library(randomForest)
library(pROC)

# Read in data and encode categorical variables
data <- read_csv("Customer Churn.csv", 
                 col_types = cols(
                   Complains = col_factor(levels = c("0", "1")), 
                   `Charge Amount` = col_factor(levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9")), 
                   `Age Group` = col_factor(levels = c("1", "2", "3", "4", "5")), 
                   `Tariff Plan` = col_factor(levels = c("1", "2")), 
                   Status = col_factor(levels = c("1", "2")), 
                   Churn = col_factor(levels = c("0", "1"))
                 )
)

# Rename columns with spaces
colnames(data) <- gsub("\\s+", "_", colnames(data))

# Checking for missing data
missing_values <- sum(is.na(data$Charge_Amount))
if (missing_values > 0) {
  # Handle missing data
  get_mode <- function(x) {
    ux <- unique(x)
    ux[which.max(tabulate(match(x, ux)))]
  }
  mode_charge_amount <- as.character(get_mode(data$Charge_Amount))
  data$Charge_Amount[is.na(data$Charge_Amount)] <- mode_charge_amount
  data$Charge_Amount <- factor(data$Charge_Amount, levels = c("0", "1", "2", "3", "4", "5", "6", "7", "8", "9"))
}

# Verify that missing values are handled
colSums(is.na(data))

# Get summary statistics of datasets
summary(data)
skim(data)

# Explore the distribution of the target variable (Churn)
summary(data$Churn)

# Visualize distribution of churn
ggplot(data, aes(x = factor(Churn))) +
  geom_bar() +
  geom_text(stat = 'count', aes(label = ..count..), vjust = -0.5) +
  labs(title = "Churn Distribution", x = "Churn", y = "Count") +
  scale_x_discrete(labels = c("Non-Churn", "Churn"))

# Visualize distribution of categorical variables
categorical_vars <- c("Complains", "Charge_Amount", "Age_Group", "Tariff_Plan", "Status")
data_long <- tidyr::gather(data, key = "Variable", value = "Value", all_of(categorical_vars))

# Bar plots for categorical variables using facet_wrap
ggplot(data_long, aes(x = factor(Value))) +
  geom_bar() +
  facet_wrap(~Variable, scales = "free_x", ncol = 2) +
  labs(title = "Distribution of Categorical Variables")

# Visualize the distribution of numerical features
numeric_vars <- c("Call_Failure", "Subscription_Length", "Seconds_of_Use", "Frequency_of_use", "Frequency_of_SMS","Distinct_Called_Numbers", "Customer_Value")

# Creating a correlation matrix to see correlated values
correlation_matrix <- cor(data[numeric_vars])
correlation_df <- melt(correlation_matrix)

# Plot the heatmap with correlation values using ggplot2
ggplot(data = correlation_df, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile(color = "white") +
  geom_text(aes(x = Var1, y = Var2, label = round(value, 2)), vjust = 1) +
  scale_fill_gradient2(low = "gold", high = "red", mid = "white", midpoint = 0, limit = c(-1, 1), space = "Lab", name = "Correlation") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1) ) +
  ggtitle("Correlation Heatmap using Iranian Churn Dataset")  

# Boxplots for numerical variables
par(mfrow = c(2, 4))
for (var in numeric_vars) {
  boxplot(data[[var]], main = var)
}

# Handling skewness and multicollinearity with Log Normalization and PCA

# Log normalization
data_numerical <- data[, numeric_vars]
skewness_before <- sapply(data_numerical, skewness)
print("Skewness before transformation:")
print(skewness_before)

for (var in numeric_vars) {
  if (skewness_before[var] > 1) {
    data_numerical[[var]] <- log1p(data_numerical[[var]])
  }
}

skewness_after <- sapply(data_numerical, skewness)
print("Skewness after transformation:")
print(skewness_after)

df_merged <- cbind(data[, setdiff(names(data), numeric_vars)], data_numerical)

# Model building after Log normalization...

# Principal Component Analysis (PCA)
pca_result <- prcomp(data_numerical, scale = TRUE)

# Plot cumulative proportion of variance explained
plot(cumsum(pca_result$sdev^2 / sum(pca_result$sdev^2)), xlab = "Number of Principal Components", ylab = "Cumulative Proportion of Variance Explained", type = "b")

n_components <- 5
pca_data <- as.data.frame(predict(pca_result, newdata = data_numerical)[, 1:n_components])

df_pca <- cbind(data[, setdiff(names(data), numeric_vars)], pca_data)

# Model building after PCA...

# ... (remaining code)

